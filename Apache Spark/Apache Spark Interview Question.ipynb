{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Describe the PySpark or Apache Spark architecture\n",
    "   - Apache Spark :=> Apache Spark is a multi-language unified analytics engine for large-scale data processing including built-in modules for SQL, data streaming, machine learning and graph processing.\n",
    "\n",
    "   - PySpark’s architecture is based on Apache Spark's distributed processing framework that follows the master-slave architecture which consists of a driver program(master node) and worker nodes. The driver program is responsible for scheduling, distributing, and monitoring tasks on the worker nodes. The driver communicates with the SparkContext to coordinate tasks and track the distributed computations. Resilient Distributed Datasets (RDDs) or DataFrames are used to represent data, which are split across nodes in the cluster for parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What are RDDs in Spark?\n",
    "\n",
    "  - RDD (Resilient Distributed Dataset) is the fundamental data structure of Apache Spark, providing an immutable, distributed collection of objects that can be processed in parallel. \n",
    "  \n",
    "  - RDDs support two types of operations: transformations (which create a new RDD) and actions (which trigger computation and return results). \n",
    "  - RDDs are fault-tolerant, allowing for efficient recovery from node failures.\n",
    "\n",
    "RDD stands for: \n",
    "\n",
    "   1. Resilient: Fault tolerant and is capable of rebuilding data on failure\n",
    "   2. Distributed: Distributed data among the multiple nodes in a cluster\n",
    "   3. Dataset: Collection of partitioned data with values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of lazy evaluation in PySpark.\n",
    "   - Lazy evaluation in PySpark means that Spark only evaluates RDDs when an action (like collect(), count(), or saveAsTextFile()) is called, not at each transformation. This allows Spark to optimize the computation by building an execution plan (or DAG) that reduces unnecessary steps and enhances efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How does PySpark differ from Apache Hadoop?\n",
    "\n",
    "   - PySpark, based on Apache Spark, processes data in-memory across distributed nodes, while Apache Hadoop processes data on disk using MapReduce. - - This in-memory processing capability makes PySpark much faster (100x faster) for iterative machine learning and data analysis tasks. PySpark also has a more versatile API and supports various data structures (e.g., DataFrames, Datasets), whereas Hadoop primarily uses key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What are DataFrames in PySpark?\n",
    "    - DataFrames in PySpark are distributed collections of data organized into named columns, similar to a table in a relational database. \n",
    "    - DataFrames are optimized for distributed data processing, providing better performance than RDDs due to Catalyst optimizer and Tungsten execution engine optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How do you initialize a SparkSession?\n",
    "\n",
    "  - A SparkSession can be initialized using the SparkSession.builder API in PySpark:\n",
    "\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MyApp\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "   - SparkSession is the entry point for working with DataFrames and executing Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What is the significance of the SparkContext?\n",
    "\n",
    "   - The SparkContext is the main entry point for Spark applications. It coordinates the distributed execution of tasks on the cluster. The SparkContext object is responsible for managing the cluster and providing access to various Spark components (RDD, broadcast variables, accumulators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Describe the types of transformations in PySpark.\n",
    "\n",
    "   - Transformations in PySpark can be narrow (e.g., map(), filter()) or wide (e.g., reduceByKey(), groupByKey()).\n",
    "   - Narrow transformations do not require data to be shuffled between nodes, while wide transformations involve shuffling and redistribution of data, which can impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. How do you read a CSV file into a PySpark DataFrame?\n",
    "\n",
    "  df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "  \n",
    "This reads a CSV file into a DataFrame, with the option to specify headers and infer data types automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What are actions in PySpark, and how do they differ from transformations?\n",
    "\n",
    "   - Actions in PySpark trigger computation and return results (e.g., collect(), count()). They differ from transformations (like map(), filter()) which define a new RDD or DataFrame but do not execute immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. How can you filter rows in a DataFrame?\n",
    "\n",
    "filtered_df = df.filter(df[\"column_name\"] > 10)\n",
    "\n",
    "This filters rows where column_name is greater than 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Explain how to perform joins in PySpark.\n",
    "\n",
    "PySpark supports multiple join types like inner, left, right, and full joins:\n",
    "\n",
    "joined_df = df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. How do you aggregate data in PySpark?\n",
    "Data can be aggregated using functions like groupBy() and agg():\n",
    "\n",
    "aggregated_df = df.groupBy(\"column\").agg({\"other_column\": \"sum\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. What are UDFs (User Defined Functions), and how are they used?\n",
    "UDFs allow users to define custom functions not available in the Spark API. They’re registered and used in DataFrames:\n",
    "\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import IntegerType\n",
    "\n",
    "    def custom_func(value):\n",
    "        return value * 2\n",
    "\n",
    "    udf_func = udf(custom_func, IntegerType())\n",
    "    df = df.withColumn(\"new_column\", udf_func(df[\"column\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. How can you handle missing or null values in PySpark?\n",
    "PySpark provides functions like na.drop() and na.fill() for handling null values:\n",
    "\n",
    "df = df.na.fill(0)  # Fill nulls with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. How do you repartition a DataFrame, and why?\n",
    "Repartitioning is used to distribute data evenly across the cluster. It’s achieved with repartition():\n",
    "\n",
    "\n",
    "df = df.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Describe how to cache a DataFrame. Why is it useful?\n",
    "DataFrames can be cached using .cache() to keep them in memory, reducing the time needed for repeated computations:\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. How do you save a DataFrame to a file?\n",
    "\n",
    "df.write.format(\"parquet\").save(\"path/to/save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. What is the Catalyst Optimizer?\n",
    "The Catalyst Optimizer in Spark SQL automatically optimizes query plans, using rules and heuristics to improve performance by reducing unnecessary computation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Explain the concept of partitioning in PySpark.\n",
    "Partitioning controls data distribution. Spark splits data into partitions to process it in parallel, enhancing scalability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. How can broadcast variables improve performance?\n",
    "Broadcast variables allow distributing a read-only variable to all nodes, reducing data transfer time and avoiding repeated data shipments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. What are accumulators, and how are they used?\n",
    "Accumulators are variables that help aggregate information from all worker nodes (e.g., counters). They’re useful for summing values in parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Describe strategies for optimizing PySpark jobs ?\n",
    "Optimizations include partitioning, avoiding shuffles, using cache, broadcast variables, Catalyst optimizer, and choosing efficient transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. What is the significance of the Tungsten execution engine?\n",
    "Tungsten improves Spark’s performance by optimizing memory management and CPU usage, providing code generation and using off-heap memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. How does PySpark handle data skewness?\n",
    "Techniques like salting (adding a random key to balance data), broadcast joins, and partition tuning are used to mitigate data skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. What are the best practices for managing memory in PySpark?\n",
    "Best practices include caching only when necessary, using broadcast joins, monitoring partition sizes, and adjusting Spark’s memory configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. How can you monitor the performance of a PySpark application?\n",
    "Monitoring can be done through Spark UI, which provides insights on job execution, resource utilization, and bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. Explain how checkpointing works in PySpark.\n",
    "Checkpointing is a process where RDDs or DataFrames are saved to a reliable storage to prevent re-computation during failure, useful in iterative algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. What is Delta Lake?\n",
    "Delta Lake is a storage layer on top of a data lake that provides ACID transactions, data versioning, and schema enforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. Explain the differences between RDD, DataFrame, and Dataset in Spark.\n",
    "   - RDD (Resilient Distributed Dataset): The fundamental data structure in Spark, offering low-level functionality and fine-grained control over data transformations. RDDs are immutable and distributed across the cluster.\n",
    "\n",
    "   - DataFrame: A higher-level abstraction over RDDs, representing data as a distributed collection of data organized into columns (like a table in a relational database). DataFrames provide optimized execution using the Catalyst optimizer and can be manipulated using SQL-like syntax.\n",
    "   \n",
    "   - Dataset: A strongly-typed, object-oriented interface that combines the benefits of RDDs and DataFrames. Datasets allow type safety in operations, making them suitable for use with complex objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. What is the purpose of caching in Spark? When should you consider caching a DataFrame?\n",
    "\n",
    "   - Caching in Spark stores the results of a DataFrame or RDD in memory, reducing the need for recomputing the data for subsequent actions. You should consider caching a DataFrame when it will be accessed multiple times in your computations, especially in iterative algorithms or when repeatedly used in actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. Explain the concept of partitioning in Spark.\n",
    "   - Partitioning refers to how data is distributed across the nodes in a Spark cluster. Spark divides large datasets into smaller partitions that are processed in parallel across the cluster. The way data is partitioned can significantly affect performance, as it impacts data locality and the need for shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. What is the difference between coalesce and repartition in Spark?\n",
    "\n",
    "   - Coalesce: Merges partitions without moving much data, useful for reducing the number of partitions (e.g., after a filter operation). It minimizes the data shuffle by avoiding unnecessary full reshuffling.\n",
    "   \n",
    "   - Repartition: Involves a full shuffle of the data and is used to increase the number of partitions, which may lead to greater resource usage. It is generally slower than coalesce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. What is the difference between cache() and persist() methods in Spark?\n",
    "   \n",
    "   - cache(): A shorthand for persisting data in memory with the default storage level of MEMORY_AND_DISK. It’s commonly used when the data is frequently reused.\n",
    "   \n",
    "   - persist(): Provides more flexibility by allowing you to specify the storage level, such as MEMORY_ONLY, MEMORY_AND_DISK, or DISK_ONLY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35. How does Spark handle schema inference for DataFrames?\n",
    " \n",
    "   - Spark automatically infers the schema of a DataFrame when reading data from structured sources like CSV, JSON, or Parquet. For CSV and JSON files, it examines a small sample of data and infers column types. Users can also define a schema explicitly by using StructType and StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 36. Describe the stages of execution in Spark.\n",
    "\n",
    "   - Stage Creation: Spark breaks down a job into smaller stages based on wide transformations (e.g., groupBy(), join()), where data shuffling is required.\n",
    "  \n",
    "   - Task Scheduling: Spark divides each stage into tasks, which are distributed across available executors.\n",
    "  \n",
    "   - Execution: Tasks are executed by Spark workers on the partitioned data, and the output is returned to the driver or further processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 37. Explain the role of DAG (Directed Acyclic Graph) in Spark's execution model.\n",
    "   \n",
    "   - The DAG in Spark represents the sequence of operations (transformations and actions) on RDDs or DataFrames. It is used to schedule and optimize jobs, ensuring that tasks are executed in the right order. The DAG allows Spark to optimize the execution plan and handle failures effectively by recovering lost data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 38. Discuss the use of window functions in Spark SQL. Provide examples of scenarios where they are beneficial.\n",
    "   \n",
    "   - Window functions allow you to perform calculations over a specified range of rows related to the current row, without collapsing them into a single row. They are useful for operations like calculating running totals, ranking, or calculating moving averages.\n",
    "\n",
    "   - Example: ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 39. How does Spark handle fault tolerance?\n",
    "\n",
    "   - Spark ensures fault tolerance through the RDD lineage mechanism. Each RDD maintains a record of the transformations applied to it, so if a partition is lost due to node failure, Spark can recompute that partition from its lineage. For fault tolerance in Spark Streaming, it uses checkpointing to save the state of the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. Explain the concept of DataFrame lineage in Spark.\n",
    "\n",
    "   - DataFrame lineage refers to the sequence of transformations applied to the data to generate the current DataFrame. Spark uses lineage information to recompute lost data partitions in case of failures, ensuring fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41.  What are broadcast variables in Spark? When and how should you use them?\n",
    "\n",
    "   - Broadcast variables allow you to efficiently share read-only variables across all nodes in a Spark cluster. They are useful when you have large lookup tables that need to be accessed by every node, reducing the cost of data transfer during joins.\n",
    "\n",
    "   - Example: sc.broadcast(small_lookup_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. Describe the different types of joins supported by Spark DataFrames.\n",
    "   \n",
    "   - Inner Join: Returns rows when there is a match in both DataFrames.\n",
    "   - Left Join: Returns all rows from the left DataFrame and matched rows from the right DataFrame.\n",
    "   - Right Join: Returns all rows from the right DataFrame and matched rows from the left DataFrame.\n",
    "   - Outer Join: Returns all rows from both DataFrames, filling in NULLs where no match is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. How does Spark perform shuffle operations in DataFrames?\n",
    "\n",
    "   - A shuffle occurs when Spark needs to redistribute data across different partitions, such as when performing a groupBy() or join(). This operation is costly as it involves disk I/O, network transfer, and memory usage. Optimizing shuffle operations (e.g., using partitioning or caching) can significantly improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44. What is the significance of checkpointing in Spark Streaming?\n",
    "\n",
    "  - Checkpointing in Spark Streaming saves the state of an RDD or DStream to a reliable storage system (e.g., HDFS). It helps recover lost data or resume processing after a failure, ensuring fault tolerance in long-running streaming applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45. How would you approach handling a large dataset that doesn't fit into memory?\n",
    "\n",
    "   - Use Spark's distributed memory: Partition the data and process it in parallel across the cluster.\n",
    "   - Persist data to disk: Use persist() with MEMORY_AND_DISK to spill data to disk when memory is insufficient.\n",
    "   - Optimize partitioning: Repartition the data to control the number of partitions and reduce memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46. How are the initial number of partitions calculated in a DataFrame?\n",
    "   \n",
    "   - The initial number of partitions is typically determined by the file size, the data source (e.g., HDFS, S3), and the configuration of Spark’s spark.default.parallelism. You can adjust the number of partitions manually using the repartition() or coalesce() methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47. Explain strategies for ensuring fault tolerance in a Spark Streaming application consuming data from Kafka.\n",
    "\n",
    "   - Checkpointing: Store offsets and the state of DStreams to Kafka topics or file systems to allow recovery in case of failures.\n",
    "   - Kafka Consumer Group: Use consumer groups to ensure that each partition is processed by only one consumer, providing fault tolerance and parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. How would you optimize joining two large datasets in Spark, with one dataset exceeding memory capacity?\n",
    "\n",
    "   - Broadcast join: If one dataset is small enough to fit in memory, broadcast it to all nodes.\n",
    "   - Partitioning: Use partitioning strategies to distribute the datasets evenly across the cluster, reducing shuffle operations.\n",
    "   - Tuning Spark memory: Adjust Spark's memory configurations to better handle large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 49. Describe the process of identifying and addressing performance bottlenecks in a slow-running Spark job.\n",
    "   \n",
    "   - Use Spark’s UI to analyze stages, tasks, and jobs. Look for skewed tasks, excessive shuffling, or out-of-memory errors. Techniques include:\n",
    "\n",
    "      - Caching frequently accessed data.\n",
    "      - Repartitioning data for better parallelism.\n",
    "      - Tuning Spark’s configuration settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. How would you distribute a large dataset efficiently across multiple nodes in a Spark cluster for parallel processing?\n",
    " \n",
    "   - Use partitioning techniques like repartition() or coalesce() to control how data is distributed across nodes. Optimize the number of partitions based on the dataset size and available cluster resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. How do you handle data skewness in Spark DataFrames? What are the implications of skewed data on performance?\n",
    "   \n",
    "   - Data skewness occurs when some partitions have much more data than others, leading to unbalanced task execution. Solutions include:\n",
    "\n",
    "      - Salting: Add random values to keys before a join to distribute the data evenly.\n",
    "      - Repartitioning: Adjust partitioning strategies based on key distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52. Explain the concept of broadcast joins in Spark. How do they work internally, and when should you use them?\n",
    "\n",
    "   - In broadcast joins, a small dataset is broadcasted to all nodes, and each node performs the join locally. This avoids expensive shuffling. \n",
    "   - Use broadcast joins when one dataset is much smaller than the other, as it minimizes network I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. Describe Spark's memory management system. How does it handle out-of-memory errors?\n",
    "\n",
    "   - Spark’s memory management system divides memory into execution and storage memory. When an out-of-memory error occurs, Spark may spill data to disk. To avoid errors, you can tune memory configurations like spark.executor.memory and spark.memory.fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. What is the difference between partitioning and bucketing in Spark?\n",
    "\n",
    "   - Partitioning: Divides data into partitions based on a column's values. It optimizes data locality.\n",
    "   - Bucketing: Divides data into a fixed number of buckets based on hash values of a column. It is used to improve join performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. Explain what happens internally when you execute spark-submit.\n",
    "\n",
    "   - When you run spark-submit, Spark loads the job code, initializes the driver program, and launches executors on worker nodes. It then schedules tasks based on the DAG, which are executed in parallel across the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 56. Discuss advanced Spark optimization techniques you have used in real-world scenarios.\n",
    "  \n",
    "   - Some advanced optimization techniques include:\n",
    "\n",
    "      - Tuning the Spark shuffle process: Optimize shuffle partitions and use custom partitioning schemes.\n",
    "      - Using DataFrame API: Leverage Spark SQL’s Catalyst optimizer for efficient execution plans.\n",
    "      - Memory management: Fine-tune executor and driver memory settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 57. How do you deploy PySpark applications in a production environment?\n",
    "\n",
    "   - Cluster Setup: You need a cluster environment (like AWS EMR, Databricks, or an on-premise cluster). Ensure the Spark cluster is set up with the necessary dependencies.\n",
    "\n",
    "   - Job Packaging: Package the PySpark application as a .zip or .tar.gz file, including the Python scripts and dependencies.\n",
    "\n",
    "   - Submit the Job: Use spark-submit to submit the job to the cluster. For example:\n",
    "      - spark-submit --master <cluster> --deploy-mode cluster --py-files dependencies.zip app.py\n",
    "\n",
    "   - Resource Management: Configure the job's resource requirements (CPU, memory) and set Spark configurations (e.g., spark.executor.memory, spark.executor.cores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 58. What are some best practices for monitoring and logging PySpark jobs?\n",
    "\n",
    "   - Use Spark UI: Monitor the job through Spark's Web UI to track stages, tasks, and job progress.\n",
    "\n",
    "   - Structured Logging: Use logging frameworks like log4j or Python's logging module to capture logs. Customize log levels (INFO, DEBUG, ERROR).\n",
    "\n",
    "   - Metrics: Track key metrics like job execution time, memory usage, and task progress using the spark.metrics configuration.\n",
    "\n",
    "   - Error Handling: Implement try-except blocks and log detailed error messages for troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 59. How do you manage resources and scheduling in a PySpark application?\n",
    "\n",
    "   - Resource Allocation: Use Spark configurations such as spark.executor.memory, spark.executor.cores, and spark.driver.memory to control memory and CPU allocation.\n",
    "\n",
    "   - Dynamic Allocation: Enable dynamic allocation by setting spark.dynamicAllocation.enabled to true to scale resources automatically.\n",
    "\n",
    "   - Cluster Manager: Use a cluster manager like YARN or Kubernetes to schedule and allocate resources for jobs. Set the job priority and resource limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60. Write a PySpark job to perform a specific data processing task (e.g., filtering data, aggregating results).\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"DataProcessingJob\").getOrCreate()\n",
    "\n",
    "    # Load data\n",
    "    df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Filter and aggregate data\n",
    "    filtered_df = df.filter(col(\"age\") > 30)\n",
    "    aggregated_df = filtered_df.groupBy(\"country\").agg({\"salary\": \"avg\"})\n",
    "\n",
    "    # Show results\n",
    "    aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 61. You have a dataset containing user activity logs with missing values and inconsistent data types. Describe how you would clean and standardize this dataset using PySpark.\n",
    "\n",
    "    # Handle Missing Values: Use fillna() to replace missing values or dropna() to remove rows with missing values.\n",
    "    df = df.fillna({\"age\": 0, \"salary\": 0.0})\n",
    "\n",
    "    # Handle Inconsistent Data Types: Use cast() to convert columns to the correct data type.\n",
    "    df = df.withColumn(\"age\", df[\"age\"].cast(\"int\"))\n",
    "\n",
    "    # Standardize Column Names: Rename columns to a consistent format using withColumnRenamed().\n",
    "    df = df.withColumnRenamed(\"userID\", \"user_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 62. Given a dataset with nested JSON structures, how would you flatten it into a tabular format using PySpark?\n",
    "   -  Use explode() to flatten arrays and selectExpr() to flatten nested fields.\n",
    "\n",
    "    from pyspark.sql.functions import explode\n",
    "\n",
    "    # Assuming 'nested_json' is a column containing a nested array\n",
    "    flattened_df = df.withColumn(\"exploded_field\", explode(df[\"nested_json\"]))\n",
    "    flattened_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 63. Your PySpark job is running slower than expected due to data skew. Explain how you would identify and address this issue.\n",
    "   \n",
    "   - Identify Skew: Check for skewed keys in the job’s execution plan using the Spark UI. Look for stages where task execution time is much higher for certain partitions.\n",
    "\n",
    "Solution:\n",
    "\n",
    "Salting: Add a random prefix to keys before performing operations like joins.\n",
    "\n",
    "    from pyspark.sql.functions import rand\n",
    "    df = df.withColumn(\"salted_key\", (df[\"key\"] + (rand() * 10).cast(\"int\")).cast(\"string\"))\n",
    "    \n",
    "    Repartitioning: Repartition the data based on key distribution.\n",
    "\n",
    "    df = df.repartition(\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
