{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('SparkLearning').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Empty RDD, DataFrame with Schema (StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create Schema\n",
    "#from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('firstName', StringType(), nullable=False),\n",
    "    StructField('middleName', StringType(), nullable=True),\n",
    "    StructField('lastName', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "\n",
    "# Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema=schema)\n",
    "df.printSchema()\n",
    "\n",
    "# Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()\n",
    "\n",
    "# Create empty DataFrame directly.\n",
    "df2 = spark.createDataFrame([], schema=schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PySpark RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dept: string (nullable = false)\n",
      " |-- Id: integer (nullable = false)\n",
      "\n",
      "+---------+---+\n",
      "|Dept     |Id |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  create an RDD by passing Python list object to sparkContext.parallelize() function\n",
    "# In PySpark, when you have a collection of data in a PySpark driver memory when we create an RDD, \"parallelized\" is going to help\n",
    "\n",
    "dept = [(\"Finance\", 10), (\"Marketing\", 20),(\"Sales\", 30), (\"IT\", 40)]\n",
    "# Creating an RDD\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "schema = StructType([\n",
    "    StructField('Dept', StringType(), nullable=False),\n",
    "    StructField('Id', IntegerType(), nullable=False)\n",
    "])\n",
    "df = rdd.toDF(schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DeptName: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      "\n",
      "+--------+---+\n",
      "|DeptName| Id|\n",
      "+--------+---+\n",
      "| Finance| 10|\n",
      "+--------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using PySpark createDataFrame() function\n",
    "dept_columns = [\"DeptName\", \"Id\"]\n",
    "df = spark.createDataFrame(rdd, schema=dept_columns)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PySpark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|  dob|gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|     James|           |    Smith|36636|     M| 60000|\n",
      "|   Michael|       Rose|         |40288|     M| 70000|\n",
      "|    Robert|           | Williams|42114|      |400000|\n",
      "|    Marria|       Anne|    Jones|39192|     F|500000|\n",
      "|       Jen|       Mary|    Brown|     |     F|     0|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"36636\", \"M\", 60000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 70000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"42114\", \"\", 400000),\n",
    "    (\"Marria\", \"Anne\", \"Jones\", \"39192\", \"F\", 500000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", 0)\n",
    "]\n",
    "\n",
    "columns = [\"first_name\", \"middle_name\", \"last_name\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3     Marria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark Dataframe to Pandas DataFrame\n",
    "pandasDF = df.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "                    name    dob gender  salary\n",
      "0       (James, , Smith)  36636      M   60000\n",
      "1      (Michael, Rose, )  40288      M   70000\n",
      "2   (Robert, , Williams)  42114         400000\n",
      "3  (Marria, Anne, Jones)  39192      F  500000\n",
      "4     (Jen, Mary, Brown)             F       0\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark Nested Struct DataFrame to Pandas\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "dataStruct = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"M\", 60000),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", 70000),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"\", 400000),\n",
    "    ((\"Marria\", \"Anne\", \"Jones\"), \"39192\", \"F\", 500000),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", 0)\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"firstname\", StringType(), nullable=True),\n",
    "        StructField(\"middlename\", StringType(), nullable=True),\n",
    "        StructField(\"lastname\", StringType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"dob\", StringType(), nullable=True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(dataStruct, schema=schemaStruct)\n",
    "df.printSchema()\n",
    "pdDF = df.toPandas()\n",
    "print(pdDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark show() â€“ Display DataFrame Contents in Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|  dob|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M| 60000|\n",
      "|   {Michael, Rose, }|40288|     M| 70000|\n",
      "|{Robert, , Williams}|42114|      |400000|\n",
      "|{Marria, Anne, Jo...|39192|     F|500000|\n",
      "|  {Jen, Mary, Brown}|     |     F|     0|\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "+---------------------+-----+------+------+\n",
      "|name                 |dob  |gender|salary|\n",
      "+---------------------+-----+------+------+\n",
      "|{James, , Smith}     |36636|M     |60000 |\n",
      "|{Michael, Rose, }    |40288|M     |70000 |\n",
      "|{Robert, , Williams} |42114|      |400000|\n",
      "|{Marria, Anne, Jones}|39192|F     |500000|\n",
      "|{Jen, Mary, Brown}   |     |F     |0     |\n",
      "+---------------------+-----+------+------+\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|name             |dob  |gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "|{James, , Smith} |36636|M     |60000 |\n",
      "|{Michael, Rose, }|40288|M     |70000 |\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|             name|  dob|gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "| {James, , Smith}|36636|     M| 60000|\n",
      "|{Michael, Rose, }|40288|     M| 70000|\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " name   | {James, , Smith}     \n",
      " dob    | 36636                \n",
      " gender | M                    \n",
      " salary | 60000                \n",
      "-RECORD 1----------------------\n",
      " name   | {Michael, Rose, }    \n",
      " dob    | 40288                \n",
      " gender | M                    \n",
      " salary | 70000                \n",
      "-RECORD 2----------------------\n",
      " name   | {Robert, , Williams} \n",
      " dob    | 42114                \n",
      " gender |                      \n",
      " salary | 400000               \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default - displays 20 rows and \n",
    "# 20 charactes from column value \n",
    "df.show()\n",
    "\n",
    "#Display full column contents\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) \n",
    "\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax\n",
    "\n",
    "def show(self, n=20, truncate=True, vertical=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark StructType & StructField\n",
    "1. PySpark provides `StructType` class from `pyspark.sql.types` to define the structure of the DataFrame.\n",
    "2. StructType is a `collection` or list of `StructField` objects.\n",
    "\n",
    "Refer cell 4 & 11 for how to create StructType & StructField with DataFrame. And also nasting of StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining schema using nested StructType\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding & Changing struct of the DataFrame\n",
    "\n",
    "Using PySpark SQL function `struct()`, we can change the struct of the existing DataFrame and add a new StructType to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- salary: integer (nullable = true)\n",
      " |    |-- Salary_grade: string (nullable = false)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                name|           OtherInfo|\n",
      "+--------------------+--------------------+\n",
      "|    {James, , Smith}|{36636, M, 3100, ...|\n",
      "|   {Michael, Rose, }|{40288, M, 4300, ...|\n",
      "|{Robert, , Williams}|{42114, M, 1400, ...|\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, ...|\n",
      "|  {Jen, Mary, Brown}|      {, F, -1, Low}|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col, when\n",
    "\n",
    "updatedDF = df2.withColumn(\n",
    "    \"OtherInfo\",\n",
    "    struct(\n",
    "        col('id').alias(\"identifier\"),\n",
    "        col('gender').alias(\"gender\"),\n",
    "        col('salary').alias(\"salary\"),\n",
    "        when(col('salary').cast(IntegerType()) < 2000, \"Low\")\n",
    "            .when(col('salary').cast(IntegerType()) < 4000, \"Medium\")\n",
    "            .otherwise('High').alias(\"Salary_grade\")\n",
    "    )\n",
    ").drop(\"id\", \"gender\", \"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL ArrayType and MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: boolean (valueContainsNull = true)\n",
      "\n",
      "-RECORD 0----------------------------------------------------\n",
      " name       | {Bob, harley, Martin}                          \n",
      " hobbies    | [Cricket, Volley]                              \n",
      " properties | {isVolleyPlayer -> true, isCricketer -> false} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL ArrayType and MapType\n",
    "from pyspark.sql.types import ArrayType, MapType, BooleanType\n",
    "\n",
    "structData = [\n",
    "    ((\"Bob\", \"harley\", \"Martin\"), [\"Cricket\", \"Volley\"], {\"isCricketer\": False, \"isVolleyPlayer\": True})\n",
    "]\n",
    "\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),BooleanType()), True)\n",
    "    ])\n",
    "\n",
    "df3 = spark.createDataFrame(structData, schema=arrayStructureSchema)\n",
    "df3.printSchema()\n",
    "df3.show(vertical=True, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating StructType object struct from JSON file\n",
    "\n",
    " You can get the schema by using `df3.schema.json()`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<name:struct<firstname:string,middlename:string,lastname:string>,hobbies:array<string>,properties:map<string,boolean>>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_json = df3.schema.json()\n",
    "df3.schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: boolean (valueContainsNull = true)\n",
      "\n",
      "-RECORD 0----------------------------------------------------\n",
      " name       | {Bob, harley, Martin}                          \n",
      " hobbies    | [Cricket, Volley]                              \n",
      " properties | {isVolleyPlayer -> true, isCricketer -> false} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now letâ€™s load the json file and use it to create a DataFrame.\n",
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(schema_json))\n",
    "df3 = spark.createDataFrame(\n",
    "        spark.sparkContext.parallelize(structData),schemaFromJson)\n",
    "df3.printSchema()\n",
    "df3.show(truncate=60, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if a Column Exists in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"name\" in df3.schema.fieldNames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Column Class\n",
    "\n",
    "1. One of the simplest ways to create a Column class object is by using PySpark `lit()` SQL function, this takes a literal value and returns a Column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "colObj = lit(\"Hello World\")\n",
    "type(colObj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     Jeams|\n",
      "|      Anna|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Jeams\", 40), (\"Anna\", 36)]\n",
    "df = spark.createDataFrame(data).toDF(\"name.fname\", \"age\")\n",
    "df.printSchema()\n",
    "\n",
    "# Using DataFrame object (df)\n",
    "df.select(df.age).show()\n",
    "df.select(df[\"age\"]).show()\n",
    "\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     Jeams|\n",
      "|      Anna|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col('age')).show()\n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with struct using Row class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(name=\"Jeams\", prop=Row(hair=\"Black\", eye=\"Blue\")),\n",
    "    Row(name=\"Ann\", prop=Row(hair=\"Grey\", eye=\"Black\"))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Jeams|\n",
      "|  Ann|\n",
      "+-----+\n",
      "\n",
      "+--------+\n",
      "|prop.eye|\n",
      "+--------+\n",
      "|    Blue|\n",
      "|   Black|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access struct column\n",
    "df.select(df.name).show()\n",
    "df.select(df.prop.eye).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|Black|\n",
      "| Grey|\n",
      "+-----+\n",
      "\n",
      "+-----+-----+\n",
      "| hair|  eye|\n",
      "+-----+-----+\n",
      "|Black| Blue|\n",
      "| Grey|Black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.select(col(\"prop.hair\")).show()\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark column also provides a way to do arithmetic operations on columns using operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      "\n",
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          201|\n",
      "|          340|\n",
      "|          591|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|          199|\n",
      "|          260|\n",
      "|          409|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|        12000|\n",
      "|        45500|\n",
      "+-------------+\n",
      "\n",
      "+------------------+\n",
      "|     (col1 / col2)|\n",
      "+------------------+\n",
      "|             200.0|\n",
      "|               7.5|\n",
      "|5.4945054945054945|\n",
      "+------------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|           20|\n",
      "|           45|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (200, 1, 2),\n",
    "    (300, 40, 20),\n",
    "    (500, 91, 42)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n",
    "df.printSchema()\n",
    "\n",
    "# Arithematic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show()\n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Column Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\",\"Bond\",\"100\",None),\n",
    "    (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "    (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "    (\"Tom Brand\",None,\"400\",'M')\n",
    "] \n",
    "\n",
    "columns = [\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "columnFuncDF = spark.createDataFrame(data,schema=columns)\n",
    "columnFuncDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alias()` â€“ Setâ€™s name to Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     James|     Bond|\n",
      "|       Ann|    Varsa|\n",
      "|Tom Cruise|      XXX|\n",
      "| Tom Brand|     NULL|\n",
      "+----------+---------+\n",
      "\n",
      "+--------------+\n",
      "|     full_name|\n",
      "+--------------+\n",
      "|    James,Bond|\n",
      "|     Ann,Varsa|\n",
      "|Tom Cruise,XXX|\n",
      "|          NULL|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# using alias()\n",
    "columnFuncDF.select(\n",
    "    columnFuncDF.fname.alias(\"first_name\"),\n",
    "    columnFuncDF.lname.alias(\"last_name\")\n",
    ").show()\n",
    "\n",
    "# Using expr()\n",
    "columnFuncDF.select(expr(\" fname ||','|| lname\").alias(\"full_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`asc()` & `desc()` â€“ Sort the DataFrame columns by Ascending or Descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort fname asc:\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|     James| Bond|100|  NULL|\n",
      "| Tom Brand| NULL|400|     M|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "Sort fname desc:\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| NULL|400|     M|\n",
      "|     James| Bond|100|  NULL|\n",
      "|       Ann|Varsa|200|     F|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc, desc to sort ascending and descending order repsectively.\n",
    "print(\"Sort fname asc:\")\n",
    "columnFuncDF.sort(columnFuncDF.fname.asc()).show()\n",
    "\n",
    "print(\"Sort fname desc:\")\n",
    "columnFuncDF.sort(columnFuncDF.fname.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cast()` & `astype()` â€“ Used to convert the data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "columnFuncDF.select(columnFuncDF.fname, columnFuncDF.id.cast(IntegerType())).printSchema()\n",
    "columnFuncDF.select(columnFuncDF.fname, columnFuncDF.id.cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`between()` â€“ Returns a Boolean expression when a column values in between lower and upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  NULL|\n",
      "+-----+-----+---+------+\n",
      "\n",
      "+----------------------------+\n",
      "|((id >= 50) AND (id <= 150))|\n",
      "+----------------------------+\n",
      "|                        true|\n",
      "|                       false|\n",
      "|                       false|\n",
      "|                       false|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.filter(columnFuncDF.id.between(50, 150)).show()\n",
    "columnFuncDF.select(columnFuncDF.id.between(50, 150)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contains()` â€“ Check if a PySpark DataFrame column value contains a string value specified in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| NULL|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.filter(columnFuncDF.fname.contains(\"Tom\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`startswith()` & `endswith()` â€“\n",
    "Checks if the value of the DataFrame Column startsWith() and endsWith() a String. startsWith() filters rows where a specified substring exists at the beginning while endsWith() filter rows where the specified substring presents at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| NULL|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+---------+-----+---+------+\n",
      "|    fname|lname| id|gender|\n",
      "+---------+-----+---+------+\n",
      "|Tom Brand| NULL|400|     M|\n",
      "+---------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.filter(columnFuncDF.fname.startswith(\"T\")).show()\n",
    "columnFuncDF.filter(columnFuncDF.fname.endswith(\"and\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isNull` & `isNotNull()` â€“ Checks if the DataFrame column has NULL or non NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  NULL|\n",
      "+-----+-----+---+------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| NULL|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.filter(columnFuncDF.gender.isNull()).show()\n",
    "columnFuncDF.filter(columnFuncDF.gender.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`like()` & `rlike()` â€“ Similar to SQL LIKE expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|    fname|lname|\n",
      "+---------+-----+\n",
      "|Tom Brand| NULL|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.select(columnFuncDF.fname, columnFuncDF.lname).filter(columnFuncDF.fname.like(\"%nd\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`substr()` â€“ Returns a Column after getting sub string from the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|substr_name|\n",
      "+-----------+\n",
      "|        Jam|\n",
      "|        Ann|\n",
      "|        Tom|\n",
      "|        Tom|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.select(columnFuncDF.fname.substr(1, 3).alias(\"substr_name\"))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`when()` & `otherwise()` â€“ It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+---+\n",
      "|     fname|lname|derived_gender| id|\n",
      "+----------+-----+--------------+---+\n",
      "|     James| Bond|          NULL|100|\n",
      "|       Ann|Varsa|        Female|200|\n",
      "|Tom Cruise|  XXX|              |400|\n",
      "| Tom Brand| NULL|          Male|400|\n",
      "+----------+-----+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "columnFuncDF.select(\n",
    "    columnFuncDF.fname, \n",
    "    columnFuncDF.lname,\n",
    "    when(columnFuncDF.gender == 'M', 'Male') \\\n",
    "    .when(columnFuncDF.gender == 'F', 'Female') \\\n",
    "    .when(columnFuncDF.gender == None, '')\n",
    "    .otherwise(columnFuncDF.gender) \\\n",
    "    .alias(\"derived_gender\"),\n",
    "    columnFuncDF.id\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isin()` â€“ Check if value presents in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "| id|fname|lname|gender|\n",
      "+---+-----+-----+------+\n",
      "|100|James| Bond|  NULL|\n",
      "|200|  Ann|Varsa|     F|\n",
      "+---+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnFuncDF.select(columnFuncDF.id, columnFuncDF.fname, columnFuncDF.lname, columnFuncDF.gender) \\\n",
    "            .filter(columnFuncDF.id.isin([100, 200])) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getField()` â€“ To get the value by key from MapType column and by stuct child name from StructType column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create DataFrame with struct, array & map\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n",
      "+---------------+\n",
      "|      languages|\n",
      "+---------------+\n",
      "|     [Java, C#]|\n",
      "| [.NET, Python]|\n",
      "|[Python, Scala]|\n",
      "|   [Perl, Ruby]|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|languages[0]|\n",
      "+------------+\n",
      "|        Java|\n",
      "|        .NET|\n",
      "|      Python|\n",
      "|        Perl|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getField from MapType\n",
    "df.select(df.properties.getField(\"hair\")).show()\n",
    "\n",
    "# getField from Struct\n",
    "df.select(df.name.getField(\"fname\")).show()\n",
    "\n",
    "# from ArrayType\n",
    "df.select(df.languages).show()\n",
    "df.select(df.languages[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getItem()` â€“ To get the value by index from MapType or ArrayTupe & ny key for MapType column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|languages[1]|\n",
      "+------------+\n",
      "|          C#|\n",
      "|      Python|\n",
      "|       Scala|\n",
      "|        Ruby|\n",
      "+------------+\n",
      "\n",
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#getItem() used with ArrayType\n",
    "df.select(df.languages.getItem(1)).show()\n",
    "\n",
    "#getItem() used with MapType\n",
    "df.select(df.properties.getItem(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Select Columns From DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, `select()` function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark `select()` is a transformation function hence it returns a new DataFrame with the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"lastname\", \"country\", \"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Single & Multiple Columns From PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\", \"lastname\", \"country\", \"state\").show()\n",
    "df.select(df.firstname, df.lastname).show()\n",
    "df.select(df['firstname'], df['lastname']).show()\n",
    "\n",
    "#By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "\n",
    "#Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select All Columns From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select All columns from List\n",
    "df.select(*columns).show()\n",
    "\n",
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|firstname|lastname|country|\n",
      "+---------+--------+-------+\n",
      "|    James|   Smith|    USA|\n",
      "|  Michael|    Rose|    USA|\n",
      "|   Robert|Williams|    USA|\n",
      "+---------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----+\n",
      "|country|state|\n",
      "+-------+-----+\n",
      "|    USA|   CA|\n",
      "|    USA|   NY|\n",
      "|    USA|   CA|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selects first 3 columns and top 3 rows\n",
    "df.select(df.columns[:3]).show(3)\n",
    "\n",
    "#Selects columns 2 to 4  and top 3 rows\n",
    "df.select(df.columns[2:4]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Nested Struct Columns from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|{James, NULL, Smith}  |OH   |M     |\n",
      "|{Anna, Rose, }        |NY   |F     |\n",
      "|{Julia, , Williams}   |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |NY   |M     |\n",
      "|{Mike, Mary, Williams}|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n",
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|{James, NULL, Smith}  |\n",
      "|{Anna, Rose, }        |\n",
      "|{Julia, , Williams}   |\n",
      "|{Maria, Anne, Jones}  |\n",
      "|{Jen, Mary, Brown}    |\n",
      "|{Mike, Mary, Williams}|\n",
      "+----------------------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|James    |NULL      |Smith   |\n",
      "|Anna     |Rose      |        |\n",
      "|Julia    |          |Williams|\n",
      "|Maria    |Anne      |Jones   |\n",
      "|Jen      |Mary      |Brown   |\n",
      "|Mike     |Mary      |Williams|\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType    \n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False) # shows all columns\n",
    "\n",
    "df2.select(\"name\").show(truncate=False)\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)\n",
    "df2.select(\"name.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
