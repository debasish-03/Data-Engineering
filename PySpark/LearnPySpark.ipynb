{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('SparkLearning').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Empty RDD, DataFrame with Schema (StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = false)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create Schema\n",
    "#from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('firstName', StringType(), nullable=False),\n",
    "    StructField('middleName', StringType(), nullable=True),\n",
    "    StructField('lastName', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "\n",
    "# Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema=schema)\n",
    "df.printSchema()\n",
    "\n",
    "# Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()\n",
    "\n",
    "# Create empty DataFrame directly.\n",
    "df2 = spark.createDataFrame([], schema=schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PySpark RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dept: string (nullable = false)\n",
      " |-- Id: integer (nullable = false)\n",
      "\n",
      "+---------+---+\n",
      "|Dept     |Id |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  create an RDD by passing Python list object to sparkContext.parallelize() function\n",
    "# In PySpark, when you have a collection of data in a PySpark driver memory when we create an RDD, \"parallelized\" is going to help\n",
    "\n",
    "dept = [(\"Finance\", 10), (\"Marketing\", 20),(\"Sales\", 30), (\"IT\", 40)]\n",
    "# Creating an RDD\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "schema = StructType([\n",
    "    StructField('Dept', StringType(), nullable=False),\n",
    "    StructField('Id', IntegerType(), nullable=False)\n",
    "])\n",
    "df = rdd.toDF(schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DeptName: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      "\n",
      "+--------+---+\n",
      "|DeptName| Id|\n",
      "+--------+---+\n",
      "| Finance| 10|\n",
      "+--------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using PySpark createDataFrame() function\n",
    "dept_columns = [\"DeptName\", \"Id\"]\n",
    "df = spark.createDataFrame(rdd, schema=dept_columns)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert PySpark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|  dob|gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|     James|           |    Smith|36636|     M| 60000|\n",
      "|   Michael|       Rose|         |40288|     M| 70000|\n",
      "|    Robert|           | Williams|42114|      |400000|\n",
      "|    Marria|       Anne|    Jones|39192|     F|500000|\n",
      "|       Jen|       Mary|    Brown|     |     F|     0|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"36636\", \"M\", 60000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 70000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"42114\", \"\", 400000),\n",
    "    (\"Marria\", \"Anne\", \"Jones\", \"39192\", \"F\", 500000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", 0)\n",
    "]\n",
    "\n",
    "columns = [\"first_name\", \"middle_name\", \"last_name\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3     Marria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark Dataframe to Pandas DataFrame\n",
    "pandasDF = df.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "                    name    dob gender  salary\n",
      "0       (James, , Smith)  36636      M   60000\n",
      "1      (Michael, Rose, )  40288      M   70000\n",
      "2   (Robert, , Williams)  42114         400000\n",
      "3  (Marria, Anne, Jones)  39192      F  500000\n",
      "4     (Jen, Mary, Brown)             F       0\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark Nested Struct DataFrame to Pandas\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "dataStruct = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"M\", 60000),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", 70000),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"\", 400000),\n",
    "    ((\"Marria\", \"Anne\", \"Jones\"), \"39192\", \"F\", 500000),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", 0)\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "    StructField(\"name\", StructType([\n",
    "        StructField(\"firstname\", StringType(), nullable=True),\n",
    "        StructField(\"middlename\", StringType(), nullable=True),\n",
    "        StructField(\"lastname\", StringType(), nullable=True)\n",
    "    ])),\n",
    "    StructField(\"dob\", StringType(), nullable=True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(dataStruct, schema=schemaStruct)\n",
    "df.printSchema()\n",
    "pdDF = df.toPandas()\n",
    "print(pdDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark show() â€“ Display DataFrame Contents in Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|  dob|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M| 60000|\n",
      "|   {Michael, Rose, }|40288|     M| 70000|\n",
      "|{Robert, , Williams}|42114|      |400000|\n",
      "|{Marria, Anne, Jo...|39192|     F|500000|\n",
      "|  {Jen, Mary, Brown}|     |     F|     0|\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "+---------------------+-----+------+------+\n",
      "|name                 |dob  |gender|salary|\n",
      "+---------------------+-----+------+------+\n",
      "|{James, , Smith}     |36636|M     |60000 |\n",
      "|{Michael, Rose, }    |40288|M     |70000 |\n",
      "|{Robert, , Williams} |42114|      |400000|\n",
      "|{Marria, Anne, Jones}|39192|F     |500000|\n",
      "|{Jen, Mary, Brown}   |     |F     |0     |\n",
      "+---------------------+-----+------+------+\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|name             |dob  |gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "|{James, , Smith} |36636|M     |60000 |\n",
      "|{Michael, Rose, }|40288|M     |70000 |\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-----+------+------+\n",
      "|             name|  dob|gender|salary|\n",
      "+-----------------+-----+------+------+\n",
      "| {James, , Smith}|36636|     M| 60000|\n",
      "|{Michael, Rose, }|40288|     M| 70000|\n",
      "+-----------------+-----+------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0----------------------\n",
      " name   | {James, , Smith}     \n",
      " dob    | 36636                \n",
      " gender | M                    \n",
      " salary | 60000                \n",
      "-RECORD 1----------------------\n",
      " name   | {Michael, Rose, }    \n",
      " dob    | 40288                \n",
      " gender | M                    \n",
      " salary | 70000                \n",
      "-RECORD 2----------------------\n",
      " name   | {Robert, , Williams} \n",
      " dob    | 42114                \n",
      " gender |                      \n",
      " salary | 400000               \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default - displays 20 rows and \n",
    "# 20 charactes from column value \n",
    "df.show()\n",
    "\n",
    "#Display full column contents\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) \n",
    "\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax\n",
    "\n",
    "def show(self, n=20, truncate=True, vertical=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark StructType & StructField\n",
    "1. PySpark provides `StructType` class from `pyspark.sql.types` to define the structure of the DataFrame.\n",
    "2. StructType is a `collection` or list of `StructField` objects.\n",
    "\n",
    "Refer cell 4 & 11 for how to create StructType & StructField with DataFrame. And also nasting of StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining schema using nested StructType\n",
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding & Changing struct of the DataFrame\n",
    "\n",
    "Using PySpark SQL function `struct()`, we can change the struct of the existing DataFrame and add a new StructType to it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- salary: integer (nullable = true)\n",
      " |    |-- Salary_grade: string (nullable = false)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                name|           OtherInfo|\n",
      "+--------------------+--------------------+\n",
      "|    {James, , Smith}|{36636, M, 3100, ...|\n",
      "|   {Michael, Rose, }|{40288, M, 4300, ...|\n",
      "|{Robert, , Williams}|{42114, M, 1400, ...|\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, ...|\n",
      "|  {Jen, Mary, Brown}|      {, F, -1, Low}|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col, when\n",
    "\n",
    "updatedDF = df2.withColumn(\n",
    "    \"OtherInfo\",\n",
    "    struct(\n",
    "        col('id').alias(\"identifier\"),\n",
    "        col('gender').alias(\"gender\"),\n",
    "        col('salary').alias(\"salary\"),\n",
    "        when(col('salary').cast(IntegerType()) < 2000, \"Low\")\n",
    "            .when(col('salary').cast(IntegerType()) < 4000, \"Medium\")\n",
    "            .otherwise('High').alias(\"Salary_grade\")\n",
    "    )\n",
    ").drop(\"id\", \"gender\", \"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL ArrayType and MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: boolean (valueContainsNull = true)\n",
      "\n",
      "-RECORD 0----------------------------------------------------\n",
      " name       | {Bob, harley, Martin}                          \n",
      " hobbies    | [Cricket, Volley]                              \n",
      " properties | {isVolleyPlayer -> true, isCricketer -> false} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL ArrayType and MapType\n",
    "from pyspark.sql.types import ArrayType, MapType, BooleanType\n",
    "\n",
    "structData = [\n",
    "    ((\"Bob\", \"harley\", \"Martin\"), [\"Cricket\", \"Volley\"], {\"isCricketer\": False, \"isVolleyPlayer\": True})\n",
    "]\n",
    "\n",
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),BooleanType()), True)\n",
    "    ])\n",
    "\n",
    "df3 = spark.createDataFrame(structData, schema=arrayStructureSchema)\n",
    "df3.printSchema()\n",
    "df3.show(vertical=True, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating StructType object struct from JSON file\n",
    "\n",
    " You can get the schema by using `df3.schema.json()`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<name:struct<firstname:string,middlename:string,lastname:string>,hobbies:array<string>,properties:map<string,boolean>>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_json = df3.schema.json()\n",
    "df3.schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- hobbies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: boolean (valueContainsNull = true)\n",
      "\n",
      "-RECORD 0----------------------------------------------------\n",
      " name       | {Bob, harley, Martin}                          \n",
      " hobbies    | [Cricket, Volley]                              \n",
      " properties | {isVolleyPlayer -> true, isCricketer -> false} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now letâ€™s load the json file and use it to create a DataFrame.\n",
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(schema_json))\n",
    "df3 = spark.createDataFrame(\n",
    "        spark.sparkContext.parallelize(structData),schemaFromJson)\n",
    "df3.printSchema()\n",
    "df3.show(truncate=60, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if a Column Exists in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"name\" in df3.schema.fieldNames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Column Class\n",
    "\n",
    "1. One of the simplest ways to create a Column class object is by using PySpark `lit()` SQL function, this takes a literal value and returns a Column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "colObj = lit(\"Hello World\")\n",
    "type(colObj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     Jeams|\n",
      "|      Anna|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Jeams\", 40), (\"Anna\", 36)]\n",
    "df = spark.createDataFrame(data).toDF(\"name.fname\", \"age\")\n",
    "df.printSchema()\n",
    "\n",
    "# Using DataFrame object (df)\n",
    "df.select(df.age).show()\n",
    "df.select(df[\"age\"]).show()\n",
    "\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 40|\n",
      "| 36|\n",
      "+---+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     Jeams|\n",
      "|      Anna|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col('age')).show()\n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with struct using Row class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(name=\"Jeams\", prop=Row(hair=\"Black\", eye=\"Blue\")),\n",
    "    Row(name=\"Ann\", prop=Row(hair=\"Grey\", eye=\"Black\"))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Jeams|\n",
      "|  Ann|\n",
      "+-----+\n",
      "\n",
      "+--------+\n",
      "|prop.eye|\n",
      "+--------+\n",
      "|    Blue|\n",
      "|   Black|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access struct column\n",
    "df.select(df.name).show()\n",
    "df.select(df.prop.eye).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|Black|\n",
      "| Grey|\n",
      "+-----+\n",
      "\n",
      "+-----+-----+\n",
      "| hair|  eye|\n",
      "+-----+-----+\n",
      "|Black| Blue|\n",
      "| Grey|Black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.select(col(\"prop.hair\")).show()\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark column also provides a way to do arithmetic operations on columns using operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      " |-- col3: long (nullable = true)\n",
      "\n",
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          201|\n",
      "|          340|\n",
      "|          591|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|          199|\n",
      "|          260|\n",
      "|          409|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|        12000|\n",
      "|        45500|\n",
      "+-------------+\n",
      "\n",
      "+------------------+\n",
      "|     (col1 / col2)|\n",
      "+------------------+\n",
      "|             200.0|\n",
      "|               7.5|\n",
      "|5.4945054945054945|\n",
      "+------------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|           20|\n",
      "|           45|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (200, 1, 2),\n",
    "    (300, 40, 20),\n",
    "    (500, 91, 42)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n",
    "df.printSchema()\n",
    "\n",
    "# Arithematic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show()\n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Column Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
