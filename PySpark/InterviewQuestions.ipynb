{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('SparkLearning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType, StructField, StructType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from InputToDataFrame import convert_input_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. [User with Most Approved Flags](https://platform.stratascratch.com/coding/2104-user-with-most-approved-flags/official-solution?code_type=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Richard', 'Hasson', 'y6120QOlsfU', '0cazx3'), ('Mark', 'May', 'Ct6BUPvE2sM', '1cn76u'), ('Gina', 'Korman', 'dQw4w9WgXcQ', '1i43zk'), ('Mark', 'May', 'Ct6BUPvE2sM', '1n0vef'), ('Mark', 'May', 'jNQXAC9IVRw', '1sv6ib'), ('Gina', 'Korman', 'dQw4w9WgXcQ', '20xekb'), ('Mark', 'May', '5qap5aO4i9A', '4cvwuv')]\n"
     ]
    }
   ],
   "source": [
    "# user_flag_data\n",
    "input_data = '''\n",
    "user_firstname\tuser_lastname\tvideo_id\tflag_id\n",
    "Richard\tHasson\ty6120QOlsfU\t0cazx3\n",
    "Mark\tMay\tCt6BUPvE2sM\t1cn76u\n",
    "Gina\tKorman\tdQw4w9WgXcQ\t1i43zk\n",
    "Mark\tMay\tCt6BUPvE2sM\t1n0vef\n",
    "Mark\tMay\tjNQXAC9IVRw\t1sv6ib\n",
    "Gina\tKorman\tdQw4w9WgXcQ\t20xekb\n",
    "Mark\tMay\t5qap5aO4i9A\t4cvwuv\n",
    "'''\n",
    "\n",
    "\n",
    "user_flags_data = convert_input_to_df(input_data)\n",
    "print(user_flags_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1cn76u', 'TRUE', '2022-03-15', 'REMOVED'), ('1i43zk', 'TRUE', '2022-03-15', 'REMOVED'), ('1n0vef', 'TRUE', '2022-03-15', 'REMOVED'), ('1sv6ib', 'TRUE', '2022-03-15', 'APPROVED'), ('20xekb', 'TRUE', '2022-03-17', 'REMOVED'), ('4cvwuv', 'TRUE', '2022-03-15', 'APPROVED')]\n"
     ]
    }
   ],
   "source": [
    "input_data = '''\n",
    "0cazx3\tFALSE\t\t\n",
    "1cn76u\tTRUE\t2022-03-15\tREMOVED\n",
    "1i43zk\tTRUE\t2022-03-15\tREMOVED\n",
    "1n0vef\tTRUE\t2022-03-15\tREMOVED\n",
    "1sv6ib\tTRUE\t2022-03-15\tAPPROVED\n",
    "20xekb\tTRUE\t2022-03-17\tREMOVED\n",
    "4cvwuv\tTRUE\t2022-03-15\tAPPROVED\t\n",
    "4sd6dv\tTRUE\t2022-03-14\tREMOVED\n",
    "6jjkvn\tTRUE\t2022-03-16\tAPPROVED\n",
    "7ks264\tTRUE\t2022-03-15\tAPPROVED\n",
    "'''\n",
    "\n",
    "flag_review_data = convert_input_to_df(input_data)\n",
    "print(flag_review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_firstname: string (nullable = true)\n",
      " |-- user_lastname: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- flag_id: string (nullable = true)\n",
      "\n",
      "+--------------+-------------+-----------+-------+\n",
      "|user_firstname|user_lastname|   video_id|flag_id|\n",
      "+--------------+-------------+-----------+-------+\n",
      "|       Richard|       Hasson|y6120QOlsfU| 0cazx3|\n",
      "|          Mark|          May|Ct6BUPvE2sM| 1cn76u|\n",
      "|          Gina|       Korman|dQw4w9WgXcQ| 1i43zk|\n",
      "|          Mark|          May|Ct6BUPvE2sM| 1n0vef|\n",
      "|          Mark|          May|jNQXAC9IVRw| 1sv6ib|\n",
      "|          Gina|       Korman|dQw4w9WgXcQ| 20xekb|\n",
      "|          Mark|          May|5qap5aO4i9A| 4cvwuv|\n",
      "+--------------+-------------+-----------+-------+\n",
      "\n",
      "==============================\n",
      "root\n",
      " |-- flag_id: string (nullable = true)\n",
      " |-- reviewed_by_yt: string (nullable = true)\n",
      " |-- reviewed_date: string (nullable = true)\n",
      " |-- reviewed_outcome: string (nullable = true)\n",
      "\n",
      "+-------+--------------+-------------+----------------+\n",
      "|flag_id|reviewed_by_yt|reviewed_date|reviewed_outcome|\n",
      "+-------+--------------+-------------+----------------+\n",
      "| 1cn76u|          TRUE|   2022-03-15|         REMOVED|\n",
      "| 1i43zk|          TRUE|   2022-03-15|         REMOVED|\n",
      "| 1n0vef|          TRUE|   2022-03-15|         REMOVED|\n",
      "| 1sv6ib|          TRUE|   2022-03-15|        APPROVED|\n",
      "| 20xekb|          TRUE|   2022-03-17|         REMOVED|\n",
      "| 4cvwuv|          TRUE|   2022-03-15|        APPROVED|\n",
      "+-------+--------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrames\n",
    "user_flags_schema = StructType([\n",
    "    StructField(\"user_firstname\", StringType(), nullable=True),\n",
    "    StructField(\"user_lastname\", StringType(), nullable=True),\n",
    "    StructField(\"video_id\", StringType(), nullable=True),\n",
    "    StructField(\"flag_id\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "flag_review_schema = StructType([\n",
    "    StructField(\"flag_id\", StringType(), nullable=True),\n",
    "    StructField(\"reviewed_by_yt\", StringType(), nullable=True),\n",
    "    StructField(\"reviewed_date\", StringType(), nullable=True),\n",
    "    StructField(\"reviewed_outcome\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "user_flags = spark.createDataFrame(user_flags_data, schema=user_flags_schema)\n",
    "user_flags.printSchema()\n",
    "user_flags.show()\n",
    "\n",
    "print(\"=\" * 30)\n",
    "\n",
    "flag_review = spark.createDataFrame(flag_review_data, schema=flag_review_schema)\n",
    "flag_review.printSchema()\n",
    "flag_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|username|\n",
      "+--------+\n",
      "|Mark May|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach :=> 1\n",
    "result = user_flags.join(flag_review, on=\"flag_id\", how=\"inner\")\n",
    "result = result.filter(F.lower(result[\"reviewed_outcome\"]) == \"approved\")\n",
    "result = result.withColumn(\"username\", F.concat(result[\"user_firstname\"], F.lit(\" \"), result[\"user_lastname\"]))\n",
    "result = result.groupby(\"username\").agg(F.countDistinct(\"video_id\").alias(\"video_count\"))\n",
    "result = result.withColumn(\"rank\", F.rank().over(Window.orderBy(F.desc(\"video_count\"))))\n",
    "result = result.filter(result[\"rank\"] == 1).select(\"username\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|username|\n",
      "+--------+\n",
      "|Mark May|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach :=> 2\n",
    "df = (\n",
    "    user_flags.join(flag_review, on=\"flag_id\", how=\"inner\")\n",
    "    .filter(F.col(\"reviewed_outcome\") == \"APPROVED\")\n",
    "    .groupBy(F.col(\"user_firstname\"), F.col(\"user_lastname\"))\n",
    "    .agg(F.countDistinct(\"video_id\").alias(\"total_video_count\"))\n",
    ")\n",
    "\n",
    "# Rank users by total_video_count and Filter to keep only the top-ranked users\n",
    "windowSpec = Window.orderBy(df[\"total_video_count\"].desc())\n",
    "df = df.withColumn(\"rank\", F.dense_rank().over(windowSpec)) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .drop(\"total_video_count\", \"rank\")\n",
    "\n",
    "\n",
    "# select all the usernames\n",
    "df = df.select(F.expr(\"user_firstname || ' ' || user_lastname\").alias(\"username\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. [Workers With The Highest Salaries](https://platform.stratascratch.com/coding/10353-workers-with-the-highest-salaries?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 1\n",
    "from  pyspark.sql.functions import col, sum, rank, desc\n",
    "from  pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "df = worker.join(title, worker.worker_id == title.worker_ref_id)\n",
    "df = df.withColumn(\"rnk\", rank().over(Window.orderBy(desc(\"salary\")))) \\\n",
    "        .where(col(\"rnk\") == 1) \\\n",
    "        .select(col(\"worker_title\").alias(\"best_paid_title\"))\n",
    "        \n",
    "\n",
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 2\n",
    "\n",
    "import  pyspark.sql.functions as F\n",
    "\n",
    "result = worker.join(title, worker.worker_id == title.worker_ref_id)\n",
    "result = result.filter(result[\"salary\"] == result.select(F.max(result[\"salary\"])).first()[0])\n",
    "result = result.select(result[\"worker_title\"]).withColumnRenamed(\"worker_title\", \"best_paid_title\")\n",
    "result.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: [Election Results](https://platform.stratascratch.com/coding/2099-election-results?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 1\n",
    "\n",
    "# Import your libraries\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Start writing code\n",
    "df = voting_results.groupBy(voting_results[\"voter\"])\\\n",
    "        .agg(F.count(voting_results[\"candidate\"]).alias(\"vote_count\"))\n",
    "\n",
    "df = df.withColumn(\"vote_value\", F.round(1.0/df[\"vote_count\"], 3))\n",
    "\n",
    "combined_df = df.join(voting_results, \"voter\") \\\n",
    "                .select(F.col(\"candidate\"), F.col(\"vote_value\"))\n",
    "\n",
    "result = combined_df.groupBy(combined_df[\"candidate\"]) \\\n",
    "            .agg(\n",
    "               F.round(F.sum(combined_df[\"vote_value\"]), 3).alias(\"total_vote_value\")) \n",
    "result = result.sort(F.col(\"total_vote_value\"), ascending = False).limit(1).select(F.col(\"candidate\"))\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 2\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate vote value for each voter\n",
    "df = voting_results.groupBy(\"voter\").agg(F.round(1.0 / F.count(\"candidate\"), 3).alias(\"vote_value\"))\n",
    "\n",
    "# Calculate net vote for each candidate\n",
    "merged_df = df.join(voting_results, \"voter\") \\\n",
    "    .filter(F.col(\"candidate\").isNotNull()) \\\n",
    "    .groupBy(\"candidate\").agg(F.sum(\"vote_value\").alias(\"net_vote\")) \\\n",
    "    .orderBy(F.desc(\"net_vote\")) \\\n",
    "    .select(\"candidate\") \\\n",
    "    .limit(1)\n",
    "\n",
    "# Show the result as a Pandas DataFrame\n",
    "merged_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 3\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate vote value for each voter\n",
    "df = voting_results.groupBy(\"voter\").agg(F.round(1.0 / F.count(\"candidate\"), 3).alias(\"vote_value\"))\n",
    "\n",
    "# Calculate net vote for each candidate\n",
    "merged_df = df.join(voting_results, \"voter\") \\\n",
    "    .filter(F.col(\"candidate\").isNotNull()) \\\n",
    "    .groupBy(\"candidate\").agg(F.sum(\"vote_value\").alias(\"net_vote\")) \\\n",
    "    .select(F.col(\"candidate\"), F.col(\"net_vote\"))\n",
    "  \n",
    "merged_df = merged_df.withColumn(\"rank\", F.dense_rank().over(Window.orderBy(F.col(\"net_vote\").desc())))\n",
    "merged_df = merged_df.filter(F.col(\"rank\") == 1).select(F.col(\"candidate\"))\n",
    "# Show the result as a Pandas DataFrame\n",
    "merged_df.show()\n",
    "\n",
    "merged_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: [Make a report showing the number of survivors and non-survivors by passenger class](https://platform.stratascratch.com/coding/9881-make-a-report-showing-the-number-of-survivors-and-non-survivors-by-passenger-class?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Start writing code\n",
    "df = titanic.select(\"survived\", \"pclass\")\n",
    "df = df.groupBy(col(\"survived\")).agg(\n",
    "        sum(when(col(\"pclass\") == 1, 1).otherwise(0)).alias(\"first_class\"),\n",
    "        sum(when(col(\"pclass\") == 2, 1).otherwise(0)).alias(\"second_class\"),\n",
    "        sum(when(col(\"pclass\") == 3, 1).otherwise(0)).alias(\"third_class\"),\n",
    "    )\n",
    "df.toPandas()\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: [Bikes Last Used](https://platform.stratascratch.com/coding/10176-bikes-last-used?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 2\n",
    "# Import your libraries\n",
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "# Start writing code\n",
    "df = dc_bikeshare_q1_2012.select(\"bike_number\", \"end_time\")\n",
    "df = df.groupBy(\"bike_number\") \\\n",
    "       .agg(max(col(\"end_time\")).alias(\"last_used\")) \\\n",
    "       .orderBy(col(\"last_used\").desc())\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach :=> 2\n",
    "# Import your libraries\n",
    "import pyspark\n",
    "from pyspark.sql import Window \n",
    "from pyspark.sql.functions import desc , row_number\n",
    "\n",
    "# Start writing code\n",
    "window = Window.partitionBy(\"bike_number\").orderBy(desc(\"end_time\"))\n",
    "df = dc_bikeshare_q1_2012.select(\"bike_number\", \"end_time\")\n",
    "df = df.withColumn(\"rnk\", row_number().over(window))\n",
    "df = df.filter(df.rnk == 1).select(df.bike_number, df.end_time.alias(\"last_time\"))\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "|  3|   C|\n",
      "|  4|   D|\n",
      "+---+----+\n",
      "\n",
      "==============================\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "|  4|   X|\n",
      "|  5|   F|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = [(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D')]\n",
    "source_columns = ['id','name']\n",
    "target_data = [(1, 'A'), (2, 'B'), (4, 'X'), (5, 'F')]\n",
    "target_columns = ['id','name']\n",
    "\n",
    "source_df = spark.createDataFrame(data=source_data, schema=source_columns)\n",
    "target_df = spark.createDataFrame(data=target_data, schema=target_columns)\n",
    "\n",
    "source_df.show()\n",
    "print(\"=\" * 30)\n",
    "target_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| Id|     comments|\n",
      "+---+-------------+\n",
      "|  3|new in source|\n",
      "|  4|    Mis-match|\n",
      "|  5|new in targer|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, coalesce\n",
    "\n",
    "df = source_df.alias(\"s\").join(target_df.alias(\"t\"), col(\"s.id\") == col(\"t.id\"), \"full\")\n",
    "df = df.select(\n",
    "        col(\"s.id\").alias(\"s_id\"),\n",
    "        col(\"s.name\").alias(\"s_name\"),\n",
    "        col(\"t.id\").alias(\"t_id\"),\n",
    "        col(\"t.name\").alias(\"t_name\"),\n",
    "    )\n",
    "df = df.withColumn(\n",
    "        \"comments\", \n",
    "        when((col(\"s_id\") == col(\"t_id\")) & (col(\"s_name\") != col(\"t_name\")), \"Mis-match\") \\\n",
    "        .when(col(\"s_id\").isNull(), \"new in targer\") \\\n",
    "        .when(col(\"t_id\").isNull(), \"new in source\")\n",
    "    )\n",
    "\n",
    "df = df.filter(df.comments.isNotNull()) \\\n",
    "    .withColumn(\"Id\", coalesce(df.s_id, df.t_id)) \\\n",
    "    .select(col(\"Id\"), col(\"comments\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: [Most Profitable Companies](https://platform.stratascratch.com/coding/10354-most-profitable-companies?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "import pyspark\n",
    "\n",
    "# Start writing code\n",
    "df = forbes_global_2010_2014\n",
    "df = df.orderBy(desc('profits')).select('company', 'profits').limit(3)\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your libraries\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# Start writing code\n",
    "df = forbes_global_2010_2014\n",
    "df = df.groupBy(df.company)\\\n",
    ".agg(F.sum(df.profits).alias(\"profits\")).orderBy(desc(\"profits\")).limit(3)\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import col, dense_rank, desc, sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Start writing code\n",
    "df = forbes_global_2010_2014\n",
    "df = df.groupBy(\"company\") \\\n",
    "        .agg(sum(\"profits\").alias(\"total_profits\"))\n",
    "\n",
    "df = df.withColumn(\"rnk\", dense_rank().over(Window.orderBy(desc(col(\"total_profits\"))))) \\\n",
    "        .where(col(\"rnk\") <= 3) \\\n",
    "        .select(col(\"company\"), col(\"total_profits\"))\n",
    "        \n",
    "        \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: [Activity Rank](https://platform.stratascratch.com/coding/10351-activity-rank?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import col, when, min, max, avg, to_date\n",
    "\n",
    "# Start writing code\n",
    "# Calculate time difference in seconds\n",
    "\n",
    "df = facebook_web_log\n",
    "df = df.where(\"action in ('page_load', 'page_exit')\") \\\n",
    "        .withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "        \n",
    "df = df.groupBy(col(\"user_id\"), col(\"date\")) \\\n",
    "      .agg(\n",
    "           max(when(col(\"action\") == \"page_load\", col(\"timestamp\"))).alias(\"load_time\"), \n",
    "           min(when(col(\"action\") == \"page_exit\", col(\"timestamp\"))).alias(\"exit_time\")\n",
    "          ) \\\n",
    "      .filter(col(\"load_time\").isNotNull() & col(\"exit_time\").isNotNull())\n",
    "      \n",
    "df = df.withColumn(\"duration\", col(\"exit_time\") - col(\"load_time\"))\n",
    "df = df.groupBy(col(\"user_id\")) \\\n",
    "        .agg(avg(col(\"duration\")).alias(\"duration\"))\n",
    "        \n",
    "df.toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = (\n",
    "    facebook_web_log\n",
    "    .filter(col('action').isin(['page_load', 'page_exit']))\n",
    "    .withColumn('timestamp', col('timestamp').cast(\"timestamp\"))\n",
    "    .groupby('user_id', date_format('timestamp', 'yyyy-MM-dd').alias('date'))\n",
    "    .agg(\n",
    "        max(when(col('action') == 'page_load', col('timestamp'))).alias('start'),\n",
    "        min(when(col('action') == 'page_exit', col('timestamp'))).alias('end')\n",
    "    )\n",
    "    .dropna()\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        mean(unix_timestamp('end') - unix_timestamp('start')).alias('duration')\n",
    "    )\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: [Activity Rank](https://platform.stratascratch.com/coding/10351-activity-rank?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Group by \"from_user\" and count total emails sent by each user\n",
    "email_counts_df = google_gmail_emails.groupBy(\"from_user\") \\\n",
    "    .agg(count(\"*\").alias(\"total_emails\"))\n",
    "\n",
    "# Rank users based on the total number of emails sent\n",
    "window_spec = Window.orderBy(col(\"total_emails\").desc(), col(\"from_user\").asc())\n",
    "ranked_emails_df = email_counts_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Display the result\n",
    "ranked_emails_df.show()\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame for validation\n",
    "ranked_emails_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: [Finding User Purchases](https://platform.stratascratch.com/coding/10322-finding-user-purchases?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, datediff\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = amazon_transactions\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"created_at\")\n",
    "df = amazon_transactions.withColumn(\"previous_purchase_date\", lag(col(\"created_at\")).over(window_spec)) \\\n",
    "                         .withColumn(\"days\", datediff(col(\"created_at\"), col(\"previous_purchase_date\")))\n",
    "                         \n",
    "df = df.where(col(\"days\") <= 7) \\\n",
    "        .select(\"user_id\").distinct()\n",
    "\n",
    "df.show()\n",
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q11: [Monthly Percentage Difference](https://platform.stratascratch.com/coding/10319-monthly-percentage-difference?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import date_format, col, sum, lag, round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Extract the year and month from the \"created_at\" column and group by it\n",
    "df = sf_transactions.withColumn(\"date\", date_format(\"created_at\", \"yyyy-MM\")) \\\n",
    "                    .groupBy(\"date\") \\\n",
    "                    .agg(\n",
    "                        sum(\"value\").alias(\"total_revenue\")\n",
    "                    )\n",
    "\n",
    "# Calculate the revenue of the previous month using lag window function\n",
    "window = Window.orderBy(\"date\")\n",
    "df = df.withColumn(\"last_month_revenue\", lag(col(\"total_revenue\")).over(window))\n",
    "\n",
    "# Calculate the month-over-month percentage change in revenue\n",
    "df = df.withColumn(\"percentage_change\", \n",
    "                   (col(\"total_revenue\") - col(\"last_month_revenue\")) / col(\"last_month_revenue\") * 100\n",
    "                  )\n",
    "\n",
    "# Round the percentage change to 2 decimal points\n",
    "df = df.withColumn(\"revenue_diff_pct\", round(col(\"percentage_change\"), 2))\n",
    "df = df.orderBy(col(\"date\")) \\\n",
    "        .select(col(\"date\"), col(\"revenue_diff_pct\"))\n",
    "# To validate your solution, convert your final PySpark DataFrame to a pandas DataFrame\n",
    "df.show()\n",
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import date_format, col, sum, lag, round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Extract the year and month from the \"created_at\" column and group by it\n",
    "window = Window.orderBy(\"date\")\n",
    "df = sf_transactions.withColumn(\"date\", date_format(\"created_at\", \"yyyy-MM\")) \\\n",
    "                    .groupBy(\"date\") \\\n",
    "                    .agg(\n",
    "                        sum(\"value\").alias(\"total_revenue\"),\n",
    "                        lag(sum(\"value\")).over(window).alias(\"last_month_revenue\")\n",
    "                    )\n",
    "\n",
    "df = df.withColumn(\"percentage_change\", \n",
    "                   (col(\"total_revenue\") - col(\"last_month_revenue\")) / col(\"last_month_revenue\") * 100\n",
    "                  )\n",
    "\n",
    "# Round the percentage change to 2 decimal points\n",
    "df = df.withColumn(\"revenue_diff_pct\", round(col(\"percentage_change\"), 2))\n",
    "df = df.orderBy(col(\"date\")) \\\n",
    "        .select(col(\"date\"), col(\"revenue_diff_pct\"))\n",
    "        \n",
    "# To validate your solution, convert your final PySpark DataFrame to a pandas DataFrame\n",
    "df.show()\n",
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q12: [New Products](https://platform.stratascratch.com/coding/10318-new-products/official-solution?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = car_launches.withColumn(\"diff\",when(col(\"year\")=='2019',-1).otherwise(1))\n",
    "df2 = df1.groupBy(\"company_name\").agg(sum(\"diff\").alias(\"net_products\"))\n",
    "df2.select(\"company_name\",\"net_products\").toPandas()\n",
    "\n",
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(col(\"company_name\")).orderBy(col(\"year\").asc())\n",
    "\n",
    "# Calculate the count of products for each company and year\n",
    "df = car_launches.groupBy(\"company_name\", \"year\") \\\n",
    "                 .agg(\n",
    "                     count(\"product_name\").alias(\"curr\"),\n",
    "                     lag(count(\"product_name\")).over(window_spec).alias(\"prev\")\n",
    "                 )\n",
    "\n",
    "# Filter the data for the year 2020 and calculate the net difference\n",
    "df = df.filter(col(\"year\") == \"2020\") \\\n",
    "      .withColumn(\"net_products\", col(\"curr\") - coalesce(col(\"prev\"), lit(0))) \\\n",
    "      .select(col(\"company_name\"), col(\"net_products\"))\n",
    "\n",
    "# Select columns and display the result\n",
    "df.show()\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame for validation\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q13: [Salaries Differences](https://platform.stratascratch.com/coding/10308-salaries-differences?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start writing code\n",
    "db_employee = db_employee.join(db_dept, db_employee['department_id'] == db_dept['id'])\\\n",
    "    .filter(db_dept['department'].isin(['engineering', 'marketing'])) \\\n",
    "    .groupby() \\\n",
    "    .pivot('department') \\\n",
    "    .agg(F.max('salary')) \\\n",
    "    .select(F.abs(F.col('engineering') - F.col('marketing')).alias('salary_difference'))\n",
    "\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "db_employee.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start writing code\n",
    "df = db_employee.join(db_dept, db_employee.department_id == db_dept.id, \"inner\")\n",
    "df = df.agg(\n",
    "        max(when(col(\"department\") == \"marketing\", col(\"salary\"))).alias(\"max_marketing_salary\"),\n",
    "        max(when(col(\"department\") == \"engineering\", col(\"salary\"))).alias(\"max_engineering_salary\")\n",
    "    ).withColumn(\"salary_difference\", \n",
    "                abs(col(\"max_marketing_salary\") - col(\"max_engineering_salary\"))\n",
    "            ).select(\"salary_difference\")\n",
    "                \n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "df.show()\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start writing code\n",
    "db_employee = db_employee \\\n",
    "    .join(db_dept, on=db_employee.department_id == db_dept.id, how=\"inner\") \\\n",
    "    .select((max(when(col(\"department\") == \"marketing\", col(\"salary\"))) - \n",
    "            max(when(col(\"department\") == \"engineering\", col(\"salary\"))))\n",
    "            .alias(\"salary_difference\"))\n",
    "\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "db_employee.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q14: [Risky Projects](https://platform.stratascratch.com/coding/10304-risky-projects?code_type=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Calculate the duration of each project in days\n",
    "df1 = linkedin_projects.withColumn(\"days\", datediff(col(\"end_date\"), col(\"start_date\")))\n",
    "\n",
    "# Join employee projects with employees\n",
    "df2 = linkedin_emp_projects.join(linkedin_employees, linkedin_emp_projects.emp_id == linkedin_employees.id)\n",
    "\n",
    "# Calculate the salary per day for each employee\n",
    "df2 = df2.withColumn(\"salary_per_day\", col(\"salary\") / 365)\n",
    "\n",
    "# Group by project_id and aggregate the total salary per day and per year\n",
    "df2 = df2.groupBy(\"project_id\").agg(\n",
    "    sum(\"salary_per_day\").alias(\"total_salary_per_day\"),\n",
    "    sum(\"salary\").alias(\"total_salary_per_year\")\n",
    ")\n",
    "\n",
    "# Join the project data with the aggregated employee data\n",
    "df = df1.join(df2, df1.id == df2.project_id)\n",
    "\n",
    "# Calculate the prorated total employee expense\n",
    "df = df.withColumn(\"prorated_expense\", col(\"total_salary_per_day\") * col(\"days\"))\n",
    "\n",
    "# Identify projects that are overbudget\n",
    "df = df.withColumn(\"overbudget\", when(col(\"prorated_expense\") > col(\"budget\"), \"Yes\").otherwise(\"No\"))\n",
    "\n",
    "# Filter projects that are overbudget\n",
    "df = df.filter(col(\"overbudget\") == \"Yes\")\n",
    "\n",
    "# Select the required columns and round the prorated expense to the nearest dollar\n",
    "df = df.select(\"title\", \"budget\", ceil(\"prorated_expense\").alias(\"prorated_expense\"))\n",
    "\n",
    "# Show the result\n",
    "df.show()\n",
    "df.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "linkedin_emp_projects = linkedin_emp_projects.join(linkedin_projects, linkedin_emp_projects[\"project_id\"]==linkedin_projects[\"id\"],\"inner\")\\\n",
    "    .select(\"project_id\",\"emp_id\", \"title\",\"budget\",\"start_date\",\"end_date\")\\\n",
    "    .join(linkedin_employees, linkedin_emp_projects[\"emp_id\"]==linkedin_employees[\"id\"])\\\n",
    "    .select(\"project_id\",\"emp_id\",\"salary\", \"title\",\"budget\",\"start_date\",\"end_date\")\n",
    "\n",
    "linkedin_emp_projects = linkedin_emp_projects.withColumn(\"duration\", datediff(col(\"end_date\"),col(\"start_date\"))/365).distinct()\n",
    "\n",
    "window = Window.partitionBy(\"title\")\n",
    "linkedin_emp_projects = linkedin_emp_projects.withColumn(\"total_sal\", ceil(sum(col(\"salary\")).over(window)*col(\"duration\")) ).select(\"title\",\"budget\", \"total_sal\").distinct().filter(col(\"total_sal\")>col(\"budget\"))\n",
    "# To validate your solution, convert your final pySpark df to a pandas df\n",
    "linkedin_emp_projects.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
